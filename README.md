
## üìÑ Descripci√≥n del proyecto

Este repositorio contiene todo el c√≥digo y artefactos desarrollados para el proyecto del TFM, enfocado en la implementaci√≥n de un pipeline de datos completo para el dataset NASA CMAPSS.

Incluye:

- **Pipeline batch (Medallion Architecture)**: C√≥digo modular en Python que implementa las fases **Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold**, preparado para empaquetarse en un archivo `.tar.gz` e importarse como librer√≠a en Azure Databricks. Junto al c√≥digo, se incluyen los notebooks utilizados para ejecutar y validar cada etapa del pipeline dentro de Databricks.

- **Pipeline streaming**: Implementaci√≥n dockerizada que simula la generaci√≥n de datos de sensores en tiempo real, envi√°ndolos a Azure Event Hubs para su posterior procesamiento y aplicaci√≥n del modelo de predicci√≥n de RUL (Remaining Useful Life) entrenado en la fase batch.

En conjunto, este repositorio re√∫ne tanto el desarrollo batch como la parte de streaming, cubriendo el ciclo completo desde la ingesta de datos hasta la generaci√≥n de alertas basadas en predicciones.

## üìÇ Estructura principal del repositorio

```
cmaps_pipeline/ # Librer√≠a Python con el c√≥digo modular del pipeline batch
‚îÇ
‚îú‚îÄ‚îÄ medallion/ # Implementaci√≥n de las fases Bronze, Silver y Gold
‚îÇ
‚îú‚îÄ‚îÄ model/ # C√≥digo relacionado con el modelo de predicci√≥n de RUL. Adjunta el modelo generado en Azure
‚îÇ
data/ # Directorio local con el dataset original en 'landing' y los directorios medallion
‚îÇ
notebooks/ # Notebooks de Databricks usados en Azure
‚îÇ
iot_generator/ # C√≥digo del productor de datos en streaming
‚îú‚îÄ‚îÄ azure_function/ # Contiene las Azure Functions usadas para enviar alertas en tiempo en real a un dominio Gmail.
‚îÇ
tests/ # Tests unitarios para los m√≥dulos del pipeline
‚îÇ
dist/ # Archivos de distribuci√≥n generados por setup.py y usados por los notebooks de Databricks
‚îÇ
Dockerfile # Dockerfile para el entorno de streaming
setup.py # Script para empaquetar e instalar la librer√≠a Python
```

---

## ‚öôÔ∏è Notas de ejecuci√≥n

Este repositorio contiene el c√≥digo real utilizado en Azure para el desarrollo del TFM, pero no es un paquete ejecutable de forma directa sobre un entorno local sin configuraci√≥n previa en la nube.  

Para prop√≥sitos de prueba, se pueden realizar las siguientes acciones:

- **Pipeline batch**:  
Es posible simular la ejecuci√≥n completa del pipeline batch de forma local mediante el script run_batch.py.
Este script ejecuta de forma secuencial las fases **Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold** usando los datos en `data/landing` como entrada y generando la salida en `data/gold`.  
En Azure, el entrenamiento del modelo se ha realizado usando el cl√∫ster de Databricks, por lo que en este repositorio √∫nicamente se incluye el c√≥digo de entrenamiento y el modelo ya entrenado exportado desde Azure.

- **Pipeline streaming**:  
El c√≥digo streaming no se puede ejecutar directamente en local sin la configuraci√≥n de **Azure Event Hubs**.  
Para pruebas, se proporciona un `Dockerfile` que empaqueta el productor de datos (`iot_generator/main.py`) y permite enviar datos simulados a Event Hubs, reproduciendo el flujo de datos en tiempo real usado en Azure.

En resumen, este repositorio es una referencia completa del desarrollo, con capacidad de prueba parcial en local (pipeline batch simulado y productor streaming dockerizado).


## üöÄ Gu√≠a de ejecuci√≥n



### Ejecutar el pipeline batch en local

* Cree y active un entorno virtual:

```bash
python -m venv venv
venv\Scripts\activate   
```

* Instale las dependencias:

```bash
pip install -r requirements.txt
```

* Ejecute el pipeline batch completo:

```bash
python run_batch.py
```

Este script leer√° los datos de data/landing, los procesar√° a trav√©s de las fases Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold y dejar√° la salida en data/gold.

### Construir y ejecutar el productor streaming (Docker)

```bash
docker build -t iot-sensor-producer .
```
Ejecutar el contenedor (conexi√≥n a Event Hubs):

```bash
docker run --rm iot-sensor-producer
```
**Nota:** El contenedor Docker se puede ejecutar en local y mostrar√° por consola los datos simulados que se estar√≠an enviando a Event Hubs. No se puede comprobar el flujo completo hacia Azure porque el Event Hub es privado. Adem√°s, el Endpoint de conexi√≥n a EventHubs se encuentra en un .env, dado que GitHub bloquea el c√≥digo con Azure Secrets"
